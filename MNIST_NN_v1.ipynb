{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data(42000,785)\n",
      "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
      "0      1       0       0       0       0       0       0       0       0   \n",
      "1      0       0       0       0       0       0       0       0       0   \n",
      "2      1       0       0       0       0       0       0       0       0   \n",
      "3      4       0       0       0       0       0       0       0       0   \n",
      "4      0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
      "0       0    ...            0         0         0         0         0   \n",
      "1       0    ...            0         0         0         0         0   \n",
      "2       0    ...            0         0         0         0         0   \n",
      "3       0    ...            0         0         0         0         0   \n",
      "4       0    ...            0         0         0         0         0   \n",
      "\n",
      "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
      "0         0         0         0         0         0  \n",
      "1         0         0         0         0         0  \n",
      "2         0         0         0         0         0  \n",
      "3         0         0         0         0         0  \n",
      "4         0         0         0         0         0  \n",
      "\n",
      "[5 rows x 785 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "\n",
    "print('data({0[0]},{0[1]})'.format(data.shape))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images(42000,784)\n"
     ]
    }
   ],
   "source": [
    "# ignore the label and flatten the pixels \n",
    "images = data.iloc[:,1:].values\n",
    "images = images.astype(np.float)\n",
    "\n",
    "# we want to normalize the pixel values\n",
    "# [0:255] => [0.0:1.0]\n",
    "images = np.multiply(images, 1.0 / 255.0 )\n",
    "\n",
    "print('images({0[0]},{0[1]})'.format(images.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images of 784 pixels, 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size => 784\n",
      "28\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "image_size = images.shape[1]\n",
    "print('image size => {0}'.format(image_size))\n",
    "\n",
    "img_wdth = img_height = np.sqrt(image_size).astype(np.uint8)\n",
    "\n",
    "print(img_wdth)\n",
    "print(img_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eff430545c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to see an image, convert flattened array into 28 * 28 image\n",
    "\n",
    "def display(img):\n",
    "    # (784) => 28 * 28\n",
    "    new_img = img.reshape(img_wdth, img_height)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(new_img, cmap=cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "display(images[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The labels provided are values for the digit in the image, 0-9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels(42000)\n",
      "[1 0 1 ..., 7 6 9]\n"
     ]
    }
   ],
   "source": [
    "labels = data[[0]].values.ravel()\n",
    "print('labels({0})'.format(len(labels)))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels count (10)\n"
     ]
    }
   ],
   "source": [
    "labels_cnt = np.unique(labels).shape[0]\n",
    "print('labels count ({0})'.format(labels_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use one-hot vectors, just an array of class values with the correct one as a 1 and the rest 0\n",
    "\n",
    "an example for a one hot vector of the digit 8\n",
    "\n",
    "[0 0 0 0 0 0 0 0 1 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 10)\n",
      "[0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# convert from scalar class label to one hot vector\n",
    "def conv_to_onehot(dense_labels, num_class):\n",
    "    num_labels = dense_labels.shape[0]\n",
    "    indx_offset = np.arange(num_labels) * num_class\n",
    "    labels_onehot = np.zeros((num_labels, num_class))\n",
    "    labels_onehot.flat[indx_offset + dense_labels.ravel()] = 1\n",
    "    return labels_onehot\n",
    "\n",
    "labels = conv_to_onehot(labels, labels_cnt)\n",
    "labels = labels.astype(np.uint8)\n",
    "\n",
    "print(labels.shape)\n",
    "print(labels[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images: (40000,784)\n",
      "validation images: (2000, 784)\n"
     ]
    }
   ],
   "source": [
    "# now split into training and validation data\n",
    "VALIDATION_SIZE = 2000\n",
    "\n",
    "validation_img = images[:VALIDATION_SIZE]\n",
    "validation_lbls = labels[:VALIDATION_SIZE]\n",
    "\n",
    "training_img = images[VALIDATION_SIZE:]\n",
    "training_lbls = labels[VALIDATION_SIZE:]\n",
    "\n",
    "print('train images: ({0[0]},{0[1]})'.format(training_img.shape))\n",
    "print('validation images: ({0[0]}, {0[1]})'.format(validation_img.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Convolutional Neural Net\n",
    "\n",
    "Now that the data is prepared, we can design the CNN\n",
    "\n",
    "First, make some helper functions to make the NN easier to work with\n",
    "\n",
    "### Helper functions\n",
    "\n",
    "We will be using ReLU for our neurons, so it is good to init with a slightly positive value for the weights so that we do not get dead neurons \n",
    "\n",
    "Also want to include some noise in the initial weights for symmetry breaking, and to prevent 0 gradient values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    init = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init)\n",
    "def bias_variable(shape):\n",
    "    init = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For information about CNN: https://en.wikipedia.org/wiki/Convolutional_neural_network#Convolutional_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convolution \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling of plain max pooling over 2x2 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pooling\n",
    "\n",
    "#[[0,2],\n",
    "#[0,3]] => 3\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of this Convolutional NN will be 2 convolutional layers with pooling in between, then a densely connected layer, followed by dropout and the readout layer to classify. Readout layer will just use simple softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input & output \n",
    "\n",
    "# images\n",
    "x = tf.placeholder('float', shape=[None, image_size])\n",
    "\n",
    "# labels\n",
    "y_ = tf.placeholder('float', shape=[None, labels_cnt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First layer is convolution with max pooling. computes 32 features from each 5 by 5 patch. Weight tensor is [5, 5, 1, 32]. 5 by 5 patch size, number of input channels (we use 1 because the images are greyscale), and number of output channels \n",
    "\n",
    "To apply layer, reshape image data into 4d tensor, with [number images, height, width, output channels] (1 for greyscale)\n",
    "\n",
    "after convolution, pooling reduces image to 28x28 to 14x14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32]) # helper function here to properly init these weight values\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "# images(4000, 784) => (4000, 28, 28, 1)\n",
    "image = tf.reshape(x, [-1, img_wdth, img_height, 1])\n",
    "# print (image.get_shape())\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(image, W_conv1) + b_conv1) # relu allows us to adjust loss as we train\n",
    "# print(h_conv1.get_shape())\n",
    "\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "# print(h_pool1.get_shape())\n",
    "\n",
    "# prepare for visualization\n",
    "\n",
    "# 32 features in 4 x 8 grid\n",
    "layer1 = tf.reshape(h_conv1, (-1, img_height, img_wdth, 4, 8))\n",
    "# print(layer1.get_shape())\n",
    "\n",
    "# reorder so channels in first dimension, x and y follow\n",
    "layer1 = tf.transpose(layer1, (0, 3, 1, 4, 2))\n",
    "# print(layer1.get_shape())\n",
    "\n",
    "layer1 = tf.reshape(layer1, (-1, img_height * 4, img_wdth * 8))\n",
    "# print(layer1.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second layer has 64 features for each 5x5 patch. Its weight tensor is shaped: [5, 5, 32, 64]\n",
    "    \n",
    "32 input channels from previous layer, 64 for our output\n",
    "\n",
    "Since the pooling has reduced to 14 x 14, this layer picks up more of the general features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "# print(h_conv2.get_shape())\n",
    "# (4000, 14, 14, 64)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# Prepare visualization\n",
    "# display 64 features in 4 * 16 grid\n",
    "layer2 = tf.reshape(h_conv2, (-1, 14, 14, 4, 6))\n",
    "\n",
    "# reorder so channels are in the first column, x and y follow\n",
    "layer2 = tf.transpose(layer2, (0, 3, 1, 4, 2))\n",
    "\n",
    "layer2 = tf.reshape(layer2, (-1, 14*4, 14*16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Image size reduced to 7 x 7, here we add fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense connected layer \n",
    "W_fcl = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fcl = bias_variable([1024])\n",
    "\n",
    "# (?, 7, 7, 64) =? (?, 3136)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "\n",
    "h_fcl = tf.nn.relu(tf.matmul(h_pool2_flat, W_fcl) + b_fcl)\n",
    "# print(h_fcl.get_shape())\n",
    "# (?, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply dropout to guard against overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropout\n",
    "keep_prob = tf.placeholder('float')\n",
    "h_fcl_drop = tf.nn.dropout(h_fcl, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the softmax layer which is the last layer before classification can be predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out layer\n",
    "W_fc2 = weight_variable([1024, labels_cnt])\n",
    "b_fc2 = bias_variable([labels_cnt])\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(h_fcl_drop, W_fc2) + b_fc2)\n",
    "# print(y.get_shape())\n",
    "# (?, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross entropy to measure loss and minimize using ADAM optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lostt\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# optimization function \n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "# evaluate \n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training, Validation, and Prediction\n",
    "\n",
    "Now that the tensorflow session structures are created and ready, we can begin to train the CNN\n",
    "\n",
    "Will use batches to train instead of all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_in_epoch = 0\n",
    "epochs_completed = 0\n",
    "num_examples = training_img.shape[0]\n",
    "\n",
    "def next_batch(batch_size):\n",
    "    \n",
    "    global training_img\n",
    "    global training_lbls\n",
    "    global index_in_epoch\n",
    "    global epochs_completed\n",
    "    \n",
    "    start = index_in_epoch\n",
    "    index_in_epoch += batch_size\n",
    "    \n",
    "    # when we have used all training data, reorder it randomly\n",
    "    if index_in_epoch > num_examples:\n",
    "        epochs_completed+=1\n",
    "        perm = np.arange(num_examples)\n",
    "        np.random.shuffle(perm)\n",
    "        training_img = training_img[perm]\n",
    "        training_lbls = training_lbls[perm]\n",
    "        \n",
    "        start = 0\n",
    "        index_in_epoch = batch_size\n",
    "    end = index_in_epoch\n",
    "    return training_img[start:end], training_lbls[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy / validation accuracy => 0.04 / 0.06 for step 0\n",
      "training accuracy / validation accuracy => 0.06 / 0.06 for step 1\n",
      "training accuracy / validation accuracy => 0.02 / 0.06 for step 2\n",
      "training accuracy / validation accuracy => 0.16 / 0.12 for step 3\n",
      "training accuracy / validation accuracy => 0.22 / 0.14 for step 4\n",
      "training accuracy / validation accuracy => 0.14 / 0.20 for step 5\n",
      "training accuracy / validation accuracy => 0.30 / 0.26 for step 6\n",
      "training accuracy / validation accuracy => 0.26 / 0.26 for step 7\n",
      "training accuracy / validation accuracy => 0.22 / 0.26 for step 8\n",
      "training accuracy / validation accuracy => 0.22 / 0.28 for step 9\n",
      "training accuracy / validation accuracy => 0.38 / 0.34 for step 10\n",
      "training accuracy / validation accuracy => 0.54 / 0.50 for step 20\n",
      "training accuracy / validation accuracy => 0.62 / 0.54 for step 30\n",
      "training accuracy / validation accuracy => 0.68 / 0.64 for step 40\n",
      "training accuracy / validation accuracy => 0.78 / 0.76 for step 50\n",
      "training accuracy / validation accuracy => 0.80 / 0.76 for step 60\n",
      "training accuracy / validation accuracy => 0.80 / 0.78 for step 70\n",
      "training accuracy / validation accuracy => 0.92 / 0.84 for step 80\n",
      "training accuracy / validation accuracy => 0.84 / 0.76 for step 90\n",
      "training accuracy / validation accuracy => 0.94 / 0.84 for step 100\n",
      "training accuracy / validation accuracy => 0.98 / 0.88 for step 200\n",
      "training accuracy / validation accuracy => 0.92 / 0.90 for step 300\n",
      "training accuracy / validation accuracy => 0.88 / 0.92 for step 400\n",
      "training accuracy / validation accuracy => 0.94 / 0.96 for step 500\n",
      "training accuracy / validation accuracy => 0.90 / 0.96 for step 600\n",
      "training accuracy / validation accuracy => 0.92 / 0.96 for step 700\n",
      "training accuracy / validation accuracy => 1.00 / 0.96 for step 800\n",
      "training accuracy / validation accuracy => 1.00 / 0.96 for step 900\n",
      "training accuracy / validation accuracy => 0.96 / 0.94 for step 1000\n",
      "training accuracy / validation accuracy => 0.96 / 0.98 for step 1999\n"
     ]
    }
   ],
   "source": [
    "# visualization variables\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "x_range = []\n",
    "\n",
    "display_step = 1\n",
    "TRAINING_ITERATIONS = 2000\n",
    "BATCH_SIZE = 50\n",
    "DROP_OUT = 0.5\n",
    "\n",
    "for i in range(TRAINING_ITERATIONS):\n",
    "    \n",
    "    # get new batch\n",
    "    batch_xs, batch_ys = next_batch(BATCH_SIZE)\n",
    "    \n",
    "    if i%display_step == 0 or (i + 1) == TRAINING_ITERATIONS:\n",
    "        \n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_xs, y_:batch_ys, keep_prob: 1.0})\n",
    "        \n",
    "        if(VALIDATION_SIZE):\n",
    "            valid_accuracy = accuracy.eval(feed_dict={ x: validation_img[0:BATCH_SIZE], \n",
    "                                                     y_:validation_lbls[0:BATCH_SIZE],\n",
    "                                                     keep_prob:1.0 })\n",
    "            print('training accuracy / validation accuracy => %.2f / %.2f for step %d' \n",
    "                                      % (train_accuracy, valid_accuracy, i))\n",
    "            valid_accuracies.append(valid_accuracy)\n",
    "        else:\n",
    "            print('training accuracy => %.4f for step %d' % (train_accuracy, i))\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        x_range.append(i)\n",
    "        \n",
    "        # increase display step\n",
    "        if i%(display_step*10)==0 and i:\n",
    "            display_step*=10\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_prob: DROP_OUT})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
